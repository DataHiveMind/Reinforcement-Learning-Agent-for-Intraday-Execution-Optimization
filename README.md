# Reinforcement-Learning-Agent-for-Intraday-Execution-Optimization

An advanced RL framework for intraday execution optimization with  
latency simulation, doubleâ€DQN, and quantitative risk analytics.

## ğŸš€ Features

- Custom Gym env with latency sampling & slippage cost  
- Double DQN with target sync, epsilon decay  
- TensorBoard logging of reward, latency, and more  
- Monte Carlo backtests: PnL distribution, VaR & CVaR  
- CLI modes: `train`, `eval`, `eval_mc`

## ğŸ“‚ Structure
rl_execution_optimizer/ 
â”œâ”€â”€ env/ # Latencyâ€aware LOB simulator 
â”œâ”€â”€ agent/ # DoubleDQN (and PG) implementations 
â”œâ”€â”€ utils/ # Metrics, logger, visualizer 
â”œâ”€â”€ train/ # Training loop with TB logging 
â”œâ”€â”€ eval/ # Singleâ€run & MonteCarlo backtests 
â”œâ”€â”€ main.py # Entry point 
â”œâ”€â”€ requirements.txt 
â””â”€â”€ README.md


## ğŸ“¦ Install

```bash
pip install -r requirements.txt

## ğŸ›  Usage
Train agent:

```bash
python main.py --mode train --data data/mock_tick_data.csv

## Evaluate a single episode:
```bash
python main.py --mode eval  --data data/mock_tick_data.csv --model double_dqn.pth

## ğŸ“Š Output
TensorBoard logs under runs/exp

PnL distribution plots via utils/visualizer.py

VaR & CVaR risk metrics